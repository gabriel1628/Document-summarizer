{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e5a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "environment_variables = dotenv_values()\n",
    "HF_TOKEN = environment_variables[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278fb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: Root=1-68049a7e-73bc044e5f888ab97cc6944a;5d7914ae-c579-4489-a0aa-050fd6196f24)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model meta-llama/Meta-Llama-3-8B-Instruct is too large to be loaded automatically (16GB > 10GB).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceClient\n\u001b[32m      3\u001b[39m client = InferenceClient(\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# base_url=...,\u001b[39;00m\n\u001b[32m      5\u001b[39m     api_key=HF_TOKEN,\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m output = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a helpful assistant.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCount to 10\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m output:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(chunk.choices[\u001b[32m0\u001b[39m].delta.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:992\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    964\u001b[39m parameters = {\n\u001b[32m    965\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: payload_model,\n\u001b[32m    966\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   (...)\u001b[39m\u001b[32m    983\u001b[39m     **(extra_body \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    984\u001b[39m }\n\u001b[32m    985\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    986\u001b[39m     inputs=messages,\n\u001b[32m    987\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    990\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    991\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    995\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _stream_chat_completion_response(data)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:357\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    354\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:473\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    468\u001b[39m     message = (\n\u001b[32m    469\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    476\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-68049a7e-73bc044e5f888ab97cc6944a;5d7914ae-c579-4489-a0aa-050fd6196f24)\n\n403 Forbidden: None.\nCannot access content at: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-8B-Instruct/v1/chat/completions.\nMake sure your token has the correct permissions.\nThe model meta-llama/Meta-Llama-3-8B-Instruct is too large to be loaded automatically (16GB > 10GB)."
     ]
    }
   ],
   "source": [
    "# source code : https://huggingface.co/docs/huggingface_hub/en/guides/inference\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    # base_url=...,\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "\n",
    "output = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Count to 10\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "for chunk in output:\n",
    "    print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bf30b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# source code : https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api\n",
    "import os\n",
    "from huggingface_hub import interpreter_login, whoami, get_token\n",
    "\n",
    "# running this will prompt you to enter your Hugging Face credentials\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a31ebdea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '67359972c88eec62944015d1',\n",
       " 'name': 'gabriel1628b',\n",
       " 'fullname': 'Gabriel Chehade',\n",
       " 'canPay': False,\n",
       " 'periodEnd': None,\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/ff27598eb9ddfdb746ad67cbeaea6654.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'MacToken',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-02-14T05:10:25.937Z',\n",
       "   'fineGrained': {'canReadGatedRepos': True,\n",
       "    'global': ['discussion.write', 'post.write'],\n",
       "    'scoped': [{'entity': {'_id': '67359972c88eec62944015d1',\n",
       "       'type': 'user',\n",
       "       'name': 'gabriel1628b'},\n",
       "      'permissions': ['repo.content.read',\n",
       "       'repo.write',\n",
       "       'inference.endpoints.infer.write',\n",
       "       'inference.endpoints.write',\n",
       "       'user.webhooks.read',\n",
       "       'user.webhooks.write',\n",
       "       'collection.read',\n",
       "       'collection.write',\n",
       "       'discussion.write',\n",
       "       'user.billing.read',\n",
       "       'inference.serverless.write']}]}}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whoami()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fa10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {get_token()}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "response = query(\n",
    "    payload={\n",
    "        \"inputs\": \"A HTTP POST request is used to \",\n",
    "        \"parameters\": {\"temperature\": 0.8, \"max_new_tokens\": 50, \"seed\": 42},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b36f792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A HTTP POST request is used to  connect to Hampe's server.  The first request is being read from the server, and the second request is being sent to the server. The first request is the HTTP POST request, and the second request is the HTTP POST request."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "Markdown(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf578b37",
   "metadata": {},
   "source": [
    "If the requested model is not already loaded into memory at the time of request (which is determined by recent requests for that model), the Serverless Inference API will initially return a 503 response, before it can successfully respond with the prediction. Try again after a few moments to allow the model time to spin up. You can also check to see which models are loaded and available at any given time using `InferenceClient().list_deployed_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25a501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'list_deployed_models' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.33.0'. HF Inference API is getting revamped and will only support warm models in the future (no cold start allowed). Use `HfApi.list_models(..., inference_provider='...')` to list warm models per provider.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text-generation': ['deepseek-ai/deepseek-coder-1.3b-instruct',\n",
       "  'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
       "  'microsoft/DialoGPT-medium',\n",
       "  'microsoft/DialoGPT-small',\n",
       "  'openai-community/gpt2',\n",
       "  'Pi3141/DialoGPT-medium-elon-2',\n",
       "  'Qwen/Qwen2.5-3B-Instruct',\n",
       "  'tiiuae/falcon-rw-1b',\n",
       "  'TinyLlama/TinyLlama-1.1B-Chat-v1.0'],\n",
       " 'feature-extraction': ['BAAI/bge-base-en-v1.5',\n",
       "  'BAAI/bge-large-en-v1.5',\n",
       "  'BAAI/bge-m3',\n",
       "  'Craig/paraphrase-MiniLM-L6-v2',\n",
       "  'facebook/bart-large',\n",
       "  'guidecare/all-mpnet-base-v2-feature-extraction',\n",
       "  'intfloat/multilingual-e5-large',\n",
       "  'keepitreal/vietnamese-sbert',\n",
       "  'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sentence-transformers/all-mpnet-base-v2',\n",
       "  'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'thenlper/gte-base-zh'],\n",
       " 'sentence-similarity': ['BAAI/bge-base-en-v1.5',\n",
       "  'BAAI/bge-large-en-v1.5',\n",
       "  'BAAI/bge-m3',\n",
       "  'Craig/paraphrase-MiniLM-L6-v2',\n",
       "  'guidecare/all-mpnet-base-v2-feature-extraction',\n",
       "  'intfloat/multilingual-e5-large',\n",
       "  'keepitreal/vietnamese-sbert',\n",
       "  'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sentence-transformers/all-mpnet-base-v2',\n",
       "  'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'thenlper/gte-base-zh'],\n",
       " 'automatic-speech-recognition': ['facebook/wav2vec2-large-xlsr-53-spanish',\n",
       "  'm3hrdadfi/wav2vec2-large-xlsr-persian-v3',\n",
       "  'msghol/whisper-large-v3-persian-common-voice-17',\n",
       "  'openai/whisper-base.en',\n",
       "  'openai/whisper-large-v2',\n",
       "  'openai/whisper-large-v3',\n",
       "  'openai/whisper-large-v3-turbo',\n",
       "  'openai/whisper-small'],\n",
       " 'fill-mask': ['FacebookAI/xlm-roberta-base'],\n",
       " 'image-classification': ['Falconsai/nsfw_image_detection',\n",
       "  'haywoodsloan/ai-image-detector-deploy',\n",
       "  'nateraw/vit-age-classifier',\n",
       "  'umm-maybe/AI-image-detector'],\n",
       " 'image-segmentation': ['jonathandinu/face-parsing'],\n",
       " 'image-to-text': ['nlpconnect/vit-gpt2-image-captioning',\n",
       "  'Salesforce/blip-image-captioning-base',\n",
       "  'Salesforce/blip-image-captioning-large'],\n",
       " 'object-detection': ['facebook/detr-resnet-50'],\n",
       " 'summarization': ['facebook/bart-large-cnn',\n",
       "  'minhtoan/t5-small-vietnamese-news',\n",
       "  'sshleifer/distilbart-cnn-12-6'],\n",
       " 'text2text-generation': ['facebook/m2m100_418M',\n",
       "  'google/flan-t5-base',\n",
       "  'google/flan-t5-large',\n",
       "  'megalaa/en-cop-mul-norm-group-greekified',\n",
       "  'megalaa/mul-cop-en-norm-group-greekified',\n",
       "  'mrm8488/t5-base-finetuned-question-generation-ap'],\n",
       " 'token-classification': ['blaze999/Medical-NER',\n",
       "  'iiiorg/piiranha-v1-detect-personal-information'],\n",
       " 'translation': ['facebook/mbart-large-50-many-to-many-mmt'],\n",
       " 'text-classification': ['cardiffnlp/twitter-roberta-base-sentiment',\n",
       "  'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
       "  'derenrich/psychiq2',\n",
       "  'facebook/roberta-hate-speech-dynabench-r4-target',\n",
       "  'jackhhao/jailbreak-classifier',\n",
       "  'KoalaAI/Text-Moderation',\n",
       "  'openai-community/roberta-base-openai-detector',\n",
       "  'SamLowe/roberta-base-go_emotions',\n",
       "  'seara/rubert-base-cased-russian-sentiment',\n",
       "  'unitary/toxic-bert'],\n",
       " 'zero-shot-classification': ['facebook/bart-large-mnli']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which models are loaded and available\n",
    "InferenceClient().list_deployed_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bc841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
