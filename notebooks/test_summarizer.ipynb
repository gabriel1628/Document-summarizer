{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30b68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "environment_variables = dotenv_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4171c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = environment_variables[\"OPENAI_API_KEY\"]\n",
    "MISTRAL_API_KEY = environment_variables[\"MISTRAL_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8518d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65575688",
   "metadata": {},
   "source": [
    "# Summarize a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d40bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from IPython.display import Markdown, display\n",
    "text = TextLoader(\"../sample_text.txt\").load()[0]\n",
    "# display(Markdown(text.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256f1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0.3, api_key=MISTRAL_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0eb7817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Write a comprehensive summary of the following text. The summary should:\n",
    "1. Highlight the main points and key ideas\n",
    "2. Include important details and supporting evidence\n",
    "3. Maintain the original meaning and intent\n",
    "4. Be well-structured and coherent\n",
    "\n",
    "Text to summarize:\n",
    "{text}\n",
    "\n",
    "Comprehensive Summary:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69111b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = chain.invoke({\"text\": text})\n",
    "# Markdown(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108efd0d",
   "metadata": {},
   "source": [
    "# Summarize multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f330d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "docs = text_splitter.create_documents([text.page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb57e1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"# Comprehensive Summary of Data Processing Engines: Ray, Dask, and Apache Spark\\n\\n## Introduction\\nThis summary provides a detailed comparison and integrated overview of three prominent data processing enginesâ€”Ray, Dask, and Apache Spark. Each engine is evaluated based on its core functionalities, performance capabilities, and ideal use cases in data science and machine learning (ML). Insights are drawn from various analyses, webinars, and blog posts discussing the evolution of machine learning tools and the importance of robust compute engines in handling large datasets and complex computations.\\n\\n## Overview of Data Processing Engines\\n\\n### Ray\\nRay is recognized for its ease of use and efficiency in managing distributed applications, particularly in reinforcement learning and deep learning tasks. It employs a master-worker architecture, enabling effective scaling with both CPUs and GPUs. Ray's unique primitives (Tasks, Objects, Actors) facilitate the distribution of Python applications, allowing for both stateless and stateful processing. Its built-in AI libraries for data preprocessing, model training, and reinforcement learning set it apart from other engines. Ray is particularly suitable for generative AI workloads and high-frequency time series forecasting.\\n\\n### Dask\\nDask is noted for its seamless integration with Python, making it a popular choice among data scientists. It allows for easy scaling of data processing tasks from single-node to distributed applications, leveraging familiar collections like DataFrames and arrays. Dask optimizes execution through a task graph, enabling parallel processing and fault tolerance. Its compatibility with popular Python libraries like NumPy and Scikit-learn enhances its appeal for data science workloads, although it is less effective for large-scale ML tasks compared to Ray and Spark.\\n\\n### Apache Spark\\nApache Spark is distinguished by its robust ecosystem and strong support for big data analytics. It excels in data pipeline resiliency and is suitable for large-scale data processing. Spark's architecture includes a driver node that manages resources and schedules tasks, optimizing performance through lazy evaluation and fault tolerance. While it supports a variety of ML tasks through MLlib, its reliance on Java can complicate debugging for Python developers. Spark is particularly effective for feature engineering and real-time ML applications.\\n\\n## Key Comparisons\\n\\n### Performance and Scalability\\n- **Ray**: Offers high cost efficiency for large-scale workloads and excels in memory-intensive tasks with autoscaling capabilities.\\n- **Dask**: Best for medium-scale tasks and lightweight exploratory data analysis but requires external resource managers for scaling.\\n- **Spark**: Known for optimized planning and fault tolerance, though it requires tuning for advanced autoscaling to prevent resource wastage.\\n\\n### Job Concurrency and Scheduling\\n- **Ray**: Features built-in gang scheduling and load-based autoscaling but does not support running multiple jobs on the same cluster.\\n- **Dask**: Can share clusters but is not recommended for multiple jobs due to its first-come-first-serve execution model.\\n- **Spark**: Offers a thread-safe scheduler that effectively manages multiple jobs within a single application, supporting FIFO and fair scheduling.\\n\\n### Memory Management and Fault Tolerance\\n- **Ray**: Utilizes a shared-memory architecture with built-in checkpointing and task retries, enhancing resilience to out-of-memory errors.\\n- **Dask**: Can spill data to disk and manage memory usage but has higher overhead due to being written in Python, with potential data loss if the scheduler fails.\\n- **Spark**: Uses RDD lineage and task rescheduling for fault tolerance, automatically retrying tasks upon failure, but requires tuning for memory-intensive workloads.\"),\n",
       " Document(metadata={}, page_content=\"### Developer Experience\\n- **Ray**: Offers a Pythonic debugging experience, though cluster debugging can be complex due to its C++ foundation.\\n- **Dask**: Entirely written in Python, making debugging familiar for developers, with comprehensive documentation.\\n- **Spark**: Mature with solid documentation but incurs overhead due to serialization between Python and JVM, complicating debugging for Python developers.\\n\\n## Use Cases in Data Science and Machine Learning\\nThe engines are evaluated based on their suitability for various data science tasks:\\n- **Scientific Computing**: Ray is recommended for scalability, while Dask supports popular Python libraries. Spark's performance can be suboptimal for specific scientific data formats.\\n- **Time Series Forecasting**: Ray is favored for high-frequency data, with Dask also being effective. Spark is used but lacks actively maintained specialized libraries.\\n- **Geospatial Data Analysis**: Ray and Dask excel in scalable analytics, while Spark can process geospatial data through Apache Sedona.\\n\\n## Language Support and Community Engagement\\nSpark natively supports Java and Scala, with strong support for SQL and R, making it a preferred choice for data engineering and large-scale ML pipelines. Ray and Dask have varying levels of community and commercial support, with Spark leading in GitHub metrics and contributions from major companies.\\n\\n## Case Studies and Real-World Applications\\nThe document discusses various case studies illustrating the practical applications of Spark, Ray, and Dask in data processing and ML. Notable examples include NVIDIA's use of Spark RAPIDS for GPU-accelerated ML, Riot Games' analytics leveraging Spark, and OpenAI's implementation of Ray for ChatGPT. These cases underscore the importance of selecting the appropriate compute engine for data science and ML.\\n\\n## Onehouse: A Unified Platform\\nOnehouse is introduced as a platform that integrates these data processing engines, providing managed ingestion and optimization capabilities. The company is actively seeking diverse, top-tier talent to shape its future.\\n\\n## Conclusion\\nIn summary, Ray, Dask, and Apache Spark each offer unique strengths that cater to different data processing needs. Ray stands out for memory-intensive and machine learning workloads, Dask is ideal for lightweight data processing and exploratory analysis, while Spark excels in large-scale data analytics with a robust ecosystem. When choosing a compute engine, practitioners should consider factors such as data scale, workload complexity, GPU needs, and team familiarity with distributed systems. The analysis encourages further research and consideration of specific use cases to make informed decisions about which engine to deploy based on organizational needs.\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4995dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "Write a concise summary of the following text, focusing on the key points:\n",
    "{text}\n",
    "\n",
    "Concise Summary:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "You are provided with multiple summaries from different sections of a document or article.\n",
    "Your task is to create a comprehensive, well-structured final summary that:\n",
    "1. Integrates all the important information from the individual summaries\n",
    "2. Presents a coherent overview of the entire content\n",
    "3. Organizes the information logically with appropriate headings and structure\n",
    "4. Eliminates redundancy while preserving important details\n",
    "\n",
    "Individual summaries:\n",
    "{text}\n",
    "\n",
    "Comprehensive Final Summary:\n",
    "\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(\n",
    "    template=map_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "355744e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9817f540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/gabriel/.local/share/uv/python/cpython-3.11.11-macos-x86_64-none/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/gabriel/.local/share/uv/python/cpython-3.11.11-macos-x86_64-none/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/gabriel/.local/share/uv/python/cpython-3.11.11-macos-x86_64-none/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3098, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3153, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3362, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3607, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/n2/76d5x8592dl18hpmjm7c4_tw0000gp/T/ipykernel_1951/502810199.py\", line 1, in <module>\n",
      "    summary = summary_chain.invoke(docs)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 160, in invoke\n",
      "    self._call(inputs, run_manager=run_manager)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py\", line 138, in _call\n",
      "    output, extra_return_dict = self.combine_docs(\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/map_reduce.py\", line 251, in combine_docs\n",
      "    result, extra_return_dict = self.reduce_documents_chain.combine_docs(\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py\", line 252, in combine_docs\n",
      "    result_docs, extra_return_dict = self._collapse(\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/reduce.py\", line 297, in _collapse\n",
      "    num_tokens = length_func(result_docs, **kwargs)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py\", line 241, in prompt_length\n",
      "    return self.llm_chain._get_num_tokens(prompt)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain/chains/llm.py\", line 409, in _get_num_tokens\n",
      "    return _get_language_model(self.llm).get_num_tokens(text)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 366, in get_num_tokens\n",
      "    return len(self.get_token_ids(text))\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 353, in get_token_ids\n",
      "    return _get_token_ids_default_method(text)\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 79, in _get_token_ids_default_method\n",
      "    tokenizer = get_tokenizer()\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 64, in get_tokenizer\n",
      "    from transformers import GPT2TokenizerFast  # type: ignore[import-not-found]\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 25, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/gabriel/Documents/Git/Document summarizer/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "summary = summary_chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07f1543f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_documents', 'output_text'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c800590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Comprehensive Summary of Data Processing Engines: Ray, Dask, and Apache Spark\n",
       "\n",
       "#### Overview\n",
       "This summary integrates key points from various sections to provide a coherent overview of three prominent data processing engines: Ray, Dask, and Apache Spark. Each engine has unique strengths, architectures, and use cases, making them suitable for different types of data processing and machine learning tasks.\n",
       "\n",
       "#### Strengths and Architectures\n",
       "\n",
       "**Ray**:\n",
       "- **Strengths**: Known for ease of use, efficient distributed applications, and strong performance in reinforcement and deep learning.\n",
       "- **Architecture**: Utilizes a master-worker model, supporting both CPUs and GPUs.\n",
       "- **Unique Features**: Includes built-in AI libraries, making it ideal for generative AI and time series forecasting.\n",
       "- **Performance**: High cost efficiency for large-scale workloads, with autoscaling capabilities.\n",
       "- **Job Concurrency**: Supports gang scheduling but does not allow multiple jobs on the same cluster.\n",
       "- **Memory Management**: Features shared-memory architecture, built-in checkpointing, and task retries.\n",
       "\n",
       "**Dask**:\n",
       "- **Strengths**: Offers seamless Python integration and easy scaling from single-node to distributed environments.\n",
       "- **Architecture**: Employs a task graph for parallel processing and fault tolerance.\n",
       "- **Compatibility**: Works well with NumPy and Scikit-learn.\n",
       "- **Performance**: Best suited for medium-scale tasks and exploratory data analysis.\n",
       "- **Job Concurrency**: Can share clusters but is not recommended for multiple jobs.\n",
       "- **Memory Management**: Can spill data to disk, but has higher overhead due to Python and potential data loss if the scheduler fails.\n",
       "\n",
       "**Apache Spark**:\n",
       "- **Strengths**: Boasts a robust ecosystem and strong support for big data analytics.\n",
       "- **Architecture**: Uses a driver node for resource management and task scheduling.\n",
       "- **Unique Features**: Implements lazy evaluation, fault tolerance, and is effective for feature engineering and real-time machine learning.\n",
       "- **Performance**: Optimized planning and fault tolerance, but requires tuning for autoscaling.\n",
       "- **Job Concurrency**: Features a thread-safe scheduler, supporting multiple jobs within a single application.\n",
       "- **Memory Management**: Utilizes RDD lineage and task rescheduling for fault tolerance, requiring tuning for memory-intensive workloads.\n",
       "\n",
       "#### Developer Experience\n",
       "\n",
       "- **Ray**: Provides Pythonic debugging but can be complex due to its C++ foundation.\n",
       "- **Dask**: Fully Python-based, offering easy debugging and comprehensive documentation.\n",
       "- **Spark**: Mature with solid documentation, but serialization overhead complicates debugging.\n",
       "\n",
       "#### Use Cases in Data Science and Machine Learning\n",
       "\n",
       "- **Scientific Computing**: Ray is preferred for scalability, Dask for Python libraries, while Spark can be suboptimal.\n",
       "- **Time Series Forecasting**: Both Ray and Dask are effective, whereas Spark lacks specialized libraries.\n",
       "- **Geospatial Data Analysis**: Ray and Dask excel, while Spark uses Apache Sedona.\n",
       "\n",
       "#### Language Support and Community Engagement\n",
       "\n",
       "- **Spark**: Supports Java, Scala, SQL, and R, with strong community and commercial support.\n",
       "- **Ray and Dask**: Vary in support levels, with Spark leading in GitHub metrics.\n",
       "\n",
       "#### Real-World Applications and Case Studies\n",
       "\n",
       "- **Examples**: NVIDIA (Spark RAPIDS), Riot Games (Spark), and OpenAI (Ray) highlight the importance of choosing the right compute engine.\n",
       "- **Onehouse Platform**: Integrates Ray, Dask, and Spark, offering managed ingestion and optimization.\n",
       "\n",
       "#### Conclusion\n",
       "\n",
       "- **Ray**: Best for memory-intensive and machine learning workloads.\n",
       "- **Dask**: Ideal for lightweight data processing and exploratory analysis.\n",
       "- **Spark**: Excels in large-scale data analytics with a robust ecosystem.\n",
       "\n",
       "Practitioners should consider the data scale, workload complexity, GPU needs, and team familiarity when selecting a compute engine. Each engine has distinct advantages tailored to specific types of data processing and machine learning tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20faff22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
