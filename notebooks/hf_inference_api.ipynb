{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23e5a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient, get_token\n",
    "import requests\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caedd7d5",
   "metadata": {},
   "source": [
    "If the requested model is not already loaded into memory at the time of request (which is determined by recent requests for that model), the Serverless Inference API will initially return a 503 response, before it can successfully respond with the prediction. Try again after a few moments to allow the model time to spin up. You can also check to see which models are loaded and available at any given time using `InferenceClient().list_deployed_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0643debe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepseek-ai/deepseek-coder-1.3b-instruct',\n",
       " 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B',\n",
       " 'distilbert/distilgpt2',\n",
       " 'ibm-granite/granite-3.3-2b-base',\n",
       " 'ismaelfaro/gpt2-poems.en',\n",
       " 'microsoft/DialoGPT-medium',\n",
       " 'microsoft/DialoGPT-small',\n",
       " 'openai-community/gpt2',\n",
       " 'Qwen/Qwen2.5-3B',\n",
       " 'Qwen/Qwen2.5-3B-Instruct',\n",
       " 'succinctly/text2image-prompt-generator',\n",
       " 'TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
       " 'uer/gpt2-chinese-cluecorpussmall']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which models are loaded and available\n",
    "InferenceClient().list_deployed_models()[\"text-generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "278fb36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure\n",
      ",\n",
      " here\n",
      " is\n",
      " a\n",
      " simple\n",
      " count\n",
      "down\n",
      " from\n",
      " \n",
      "1\n",
      "0\n",
      " to\n",
      " \n",
      "1\n",
      " in\n",
      "clus\n",
      "ively\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "0\n",
      ",\n",
      " \n",
      "9\n",
      ",\n",
      " \n",
      "8\n",
      ",\n",
      " \n",
      "7\n",
      ",\n",
      " \n",
      "6\n",
      ",\n",
      " \n",
      "5\n",
      ",\n",
      " \n",
      "4\n",
      ",\n",
      " \n",
      "3\n",
      ",\n",
      " \n",
      "2\n",
      ",\n",
      " \n",
      "1\n",
      ",\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Happy\n",
      " counting\n",
      "!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# source code : https://huggingface.co/docs/huggingface_hub/en/guides/inference\n",
    "\n",
    "client = InferenceClient(\n",
    "    # base_url=...,\n",
    "    api_key=get_token(),\n",
    ")\n",
    "\n",
    "\n",
    "output = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/deepseek-coder-1.3b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Count to 10\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "for chunk in output:\n",
    "    print(chunk.choices[0].delta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2fa10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code : https://huggingface.co/learn/cookbook/en/enterprise_hub_serverless_inference_api\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/openai-community/gpt2\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer {get_token()}\"}\n",
    "\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "response = query(\n",
    "    payload={\n",
    "        \"inputs\": \"A HTTP POST request is used to \",\n",
    "        \"parameters\": {\"temperature\": 0.8, \"max_new_tokens\": 50, \"seed\": 42},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b36f792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "A HTTP POST request is used to  connect to Hampe's server.  The first request is being read from the server, and the second request is being sent to the server. The first request is the HTTP POST request, and the second request is the HTTP POST request."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bc841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
